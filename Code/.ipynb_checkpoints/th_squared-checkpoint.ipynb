{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODULES,  ALLIASES AND GLOBAL ATTRIBUTES\n",
    "# =============================================================================\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "from googletrans import Translator\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Global attributes\n",
    "api_key = 'insert your own API KEY here'\n",
    "url = \"https://maps.googleapis.com/maps/api/place/textsearch/json?\"\n",
    "# Aliases\n",
    "join=os.path.join\n",
    "make=os.makedirs\n",
    "exists=os.path.exists\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA FETCHING & ONLINE PREPROCESSING\n",
    "# =============================================================================\n",
    "def construct_search_query(key='accountant',town='Xanthi'):\n",
    "    '''\n",
    "    Create a search query based on the venue of interest and the town. \n",
    "    Translate the query in Greek to fetch venues with only Greek domains.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    key: STR, \n",
    "        Provide the search target, optional.\n",
    "        The default is 'accountant'.\n",
    "\n",
    "    town: STR, optional\n",
    "        Provide the name of the town that you'd wish to perform the search.\n",
    "        The default is 'Xanthi'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    query: LIST.\n",
    "        A list of all possible queries in English and Greek using the \n",
    "        provided parameters. The list includes capitalized and upper versions\n",
    "        of the queries in both languages.\n",
    "\n",
    "    '''\n",
    "    translator = Translator()\n",
    "\n",
    "    # To extend the search space create queries with and without determiners\n",
    "    english_queries=[]\n",
    "    for comb in range(0,6): #comb=combinations\n",
    "        if comb==0:\n",
    "            # simply combine with space\n",
    "            tmp_query=(key+' '+town).strip()\n",
    "        elif comb==1:\n",
    "            # add a preposition\n",
    "            tmp_query=(key+' in '+town).strip()\n",
    "        elif comb==2:\n",
    "            # capitalize each word\n",
    "            tmp_query=(key.capitalize()+' '+town.capitalize()).strip()\n",
    "        elif comb==3:\n",
    "            # capitalize each word\n",
    "            tmp_query=(key.capitalize()+' in '+town.capitalize()).strip()\n",
    "        elif comb==4:\n",
    "            # make upper\n",
    "            tmp_query=(key.upper()+' in '+town.upper()).strip()   \n",
    "        elif comb==5:\n",
    "            # make upper\n",
    "            tmp_query=(key.upper()+' '+town.upper()).strip()   \n",
    "        english_queries.append(tmp_query)\n",
    "    \n",
    "\n",
    "    # Now translate the English queries to Greek and construct the search list\n",
    "        \n",
    "    translations = translator.translate(english_queries, dest='greek')\n",
    "    greek_queries=[translations[i].text for i in range(0,len(translations))]\n",
    "    # combine the queries from both languages\n",
    "    queries=english_queries + greek_queries\n",
    "\n",
    "    return queries\n",
    "\n",
    "def fetch_data(queries,key='accountant',town='Xanthi'):\n",
    "    '''\n",
    "    Using the queries created with f'construct_search_query', \n",
    "    return the features of interest using the Google Places API.    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    queries : LIST\n",
    "        Queries and variants in English and Greek.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature_matrix: DATAFRAME\n",
    "    Contains the name, rating and coordinates of the place\n",
    "\n",
    "    '''\n",
    "\n",
    "    # features of interest (foi)\n",
    "    foi=['name', 'user_ratings_total','rating', 'latitude', 'longitude']\n",
    "    # Initialize containers for features of interest\n",
    "    names, user_ratings_total,rating, rating, latitude, longitude=\\\n",
    "        ([] for i in range(0,6))\n",
    "    \n",
    "    for query in tqdm(queries):    \n",
    "        # return response object \n",
    "        r = requests.get(url + 'query=' + query +\n",
    "\t\t\t\t\t\t'&key=' + api_key) \n",
    "        x = r.json()\n",
    "        \n",
    "        for result in range(0,len(x['results'])):\n",
    "            names.append(x['results'][result]['name'])\n",
    "            try:\n",
    "                user_ratings_total.append(x['results'][result]['user_ratings_total'])\n",
    "            except:\n",
    "                print(f'{key, user_ratings_total} not found, setting to NaN')\n",
    "                user_ratings_total.append(np.nan)\n",
    "            try:    \n",
    "                rating.append(x['results'][result]['rating'])\n",
    "            except:\n",
    "                print(f'{key, rating} not found, setting to NaN')\n",
    "                rating.append(np.nan)\n",
    "            latitude.append(x['results'][result]['geometry']['location']['lat'])\n",
    "            longitude.append(x['results'][result]['geometry']['location']['lng'])\n",
    "            \n",
    "\n",
    "    # Construct dataframe\n",
    "    df=pd.DataFrame(columns=foi)    \n",
    "    df.name=names\n",
    "    df.user_ratings_total=user_ratings_total\n",
    "    df.rating=rating\n",
    "    df.latitude=latitude\n",
    "    df.longitude=longitude\n",
    "\n",
    "    # Now remove duplicates\n",
    "    df = df.drop_duplicates(subset='name', keep=\"first\")\n",
    "    # Save the dataframe locally\n",
    "    path2data=join(os.path.realpath('..'),'Data', town)\n",
    "    if not exists(path2data): make(path2data)\n",
    "    fname=join(path2data,key+'_'+town+'.csv')\n",
    "    df.to_csv(fname, encoding='utf-8-sig')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "####################################################################\n",
    "#   Fetch data for accountants, lawyers, and banks\n",
    "####################################################################\n",
    "keys=['accountant', 'lawyer','bank', 'insurer']\n",
    "for town in ['Xanthi','Thessaloniki', 'Athens','heraklion', 'Patras']:\n",
    "    for key in keys:\n",
    "        print(key)\n",
    "        # Get queries in English and Greek\n",
    "        queries=construct_search_query(key,town)\n",
    "        # Store preprocessed dataframes\n",
    "        fetch_data(queries,key,town)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "# Path declaration\n",
    "path2figs=join(os.path.realpath('..'),'Figures')\n",
    "\n",
    "# =============================================================================\n",
    "towns=['Athens','heraklion','Patras','Thessaloniki','Xanthi']\n",
    "target_venues=['lawyer', 'bank', 'accountant', 'insurer']\n",
    "\n",
    "def load_data(town):\n",
    "    # == load .csvs == #\n",
    "    path2data=join(os.path.realpath('..'),'Data', town)\n",
    "    files=see(path2data)\n",
    "    data={} # initialize dictionary to hold the data\n",
    "    for idx,file in enumerate(files):\n",
    "        target_name=files[idx].split('_')[0] # eg: accountant\n",
    "        data[target_name]=load(join(path2data, file))\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# EXPLORATORY DATA ANALYSIS\n",
    "# =============================================================================\n",
    "def plot_dist_of_total_ratings(town, data):\n",
    "    # Q1:  Can we use the rating as a reliable factor??\n",
    "    curr_path2figs=join(path2figs, '#n_ratings',town)\n",
    "    if not exists(curr_path2figs): make(curr_path2figs)\n",
    "\n",
    "    for target in target_venues:\n",
    "        sns.distplot(data[target].user_ratings_total, hist=False, kde=True, label=target)\n",
    "    plt.legend()\n",
    "    plt.title(f'{town}')\n",
    "    plt.ylabel('Gaussian kernel density estimate')\n",
    "    plt.xlabel('#ratings')\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    figname=join(curr_path2figs,f'{town}_total_ratings.png')\n",
    "    plt.savefig(figname, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "for town in towns: \n",
    "    # load data\n",
    "    data=load_data(town)\n",
    "    # plot the total number of ratings\n",
    "    plot_dist_of_total_ratings(town, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PREPROCESSING - OUTLIER DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def plot_mean_longitude(data,state):\n",
    "\n",
    "    longs=[np.mean(data[target].longitude) for target in target_venues ]\n",
    "    error=[np.std(data[target].longitude) for target in target_venues ]\n",
    "    x_pos=np.arange(0,len(target_venues))\n",
    "    # 2. We need to remove outliers from the lawyer and bank offices\n",
    "    # Build the plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x_pos, longs, yerr=error, align='center', alpha=0.2, ecolor='black', capsize=10)\n",
    "    ax.set_ylabel('Longitude')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(target_venues)\n",
    "    plt.title('Longitude means & STDs')\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    figname=join(path2figs,f'longitude_{state}_.png')\n",
    "    plt.savefig(figname, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# %%\n",
    "town='Athens'\n",
    "# focus on a single town  \n",
    "data=load_data(town)\n",
    "# plot the means before the outlier removal \n",
    "plot_mean_longitude(data,'before_pre')\n",
    "# remove outliers\n",
    "remove_outliers_from=['bank', 'lawyer']\n",
    "for venue in remove_outliers_from:\n",
    "    data[venue].drop(data[venue][data[venue].longitude<10].index, inplace=True)\n",
    "# plot again to verify    \n",
    "plot_mean_longitude(data,'after_pre')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "colors=['g','r','b','m']\n",
    "\n",
    "for venue, color in zip(target_venues,colors):\n",
    "    curr_data=data[venue]\n",
    "    # curr_data=target_bad_ratings_area(data, venue, thr)\n",
    "    \n",
    "    plt.scatter(curr_data.latitude, curr_data.longitude, color=color,\n",
    "                # s=1e1*data[venue].rating,\n",
    "                label=venue)\n",
    "plt.title(town)\n",
    "plt.legend()\n",
    "sns.despine(offset=10, trim=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLUSTERING\n",
    "# =============================================================================\n",
    "# Import model\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "distance=sklearn.metrics.pairwise.euclidean_distances\n",
    "\n",
    "centroids_coords={}\n",
    "\n",
    "\n",
    "for idx, venue in enumerate(target_venues):\n",
    "    curr_venue=data[venue][['latitude','longitude']]\n",
    "    \n",
    "    \n",
    "    # per venue use the silhouette_score and select the optimal k \n",
    "    sil = []\n",
    "    kmax = 10\n",
    "    for k in range(2, kmax+1):\n",
    "      kmeans = KMeans(n_clusters = k).fit(curr_venue)\n",
    "      labels = kmeans.labels_\n",
    "      sil.append(silhouette_score(curr_venue, labels, metric = 'euclidean'))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=np.argmax(sil)+1).fit(curr_venue)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    centroids_coords[venue]={}\n",
    "    centroids_coords[venue]['coords']=centroids\n",
    "    \n",
    "    plt.subplot(2,2,idx+1)\n",
    "    plt.scatter(curr_venue['latitude'], curr_venue['longitude'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)\n",
    "    plt.title(f'{venue, town}')\n",
    "    sns.despine(offset=10, trim=False)\n",
    "    plt.ylim(23.67,23.82 )\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "plt.suptitle('K-means clustered coordinates for all venues of interest. ', y=1.05)\n",
    "fig.set_size_inches(10, 6)\n",
    "plt.show()\n",
    "      \n",
    "#%%\n",
    "for idx, venue in enumerate(target_venues):\n",
    "    centroids=centroids_coords[venue]['coords']\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], c=colors[idx], s=50, label=venue)    \n",
    "\n",
    "plt.title('Spatial distribution of venues-centroids for the town of Athens', y=1.2, style='oblique')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.25),\n",
    "          ncol=2, fancybox=True, shadow=True, title='Professions.')\n",
    "plt.xlabel(r'$\\mathcal{latitude}$')\n",
    "plt.ylabel(r'$\\mathcal{longitude}$')\n",
    "sns.despine(offset=10, trim=False)\n",
    "# plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "def parse_synergetic_and_antagonistic_elements(centroids_coords):\n",
    "    ## SYNERGETIC ELEMENTS\n",
    "    synergetic_lat=[centroids_coords[cat]['coords'][:,0] for cat in ['bank']]\n",
    "    synergetic_lat = np.array([item for sublist in synergetic_lat for item in sublist])\n",
    "\n",
    "\n",
    "    # Antagonistic elements longtitude\n",
    "    synergetic_long=[centroids_coords[cat]['coords'][:,1] for cat in ['bank']]\n",
    "    synergetic_long = np.array([item for sublist in synergetic_long for item in sublist])\n",
    "      \n",
    "    \n",
    "    ## ANTAGONISTIC ELEMENTS\n",
    "    # Antagonistic elements latitude\n",
    "    antagonistic_lat=[centroids_coords[cat]['coords'][:,0] for cat in ['lawyer', 'accountant', 'insurer']]\n",
    "    antagonistic_lat = np.array([item for sublist in antagonistic_lat for item in sublist])\n",
    "\n",
    "    # Antagonistic elements longtitude\n",
    "    antagonistic_long=[centroids_coords[cat]['coords'][:,1] for cat in ['lawyer', 'accountant', 'insurer']]\n",
    "    antagonistic_long = np.array([item for sublist in antagonistic_long for item in sublist])\n",
    "\n",
    "    elements={}\n",
    "    for item in ['syn','ant']: elements[item]={}\n",
    "    elements['syn']['lat'] =synergetic_lat\n",
    "    elements['syn']['long']=synergetic_long    \n",
    "\n",
    "    elements['ant']['lat'] =antagonistic_lat\n",
    "    elements['ant']['long']=antagonistic_long    \n",
    "\n",
    "    return elements\n",
    "\n",
    "def plot_superimposed_grid(elements, candidate_lat, canditate_long, mean_lat, mean_long):\n",
    "    \n",
    "    plt.scatter(elements['ant']['lat'], elements['ant']['long'],  label='antagonistic elements', s=50, edgecolors='k')\n",
    "    plt.scatter(elements['syn']['lat'], elements['syn']['long'], color='red', label='synergetic elements', s=50, edgecolors='k')\n",
    "    plt.scatter(candidate_lat,canditate_long, marker='.', color='k', alpha=0.5, label='candidate coords', zorder=0)  \n",
    "    plt.scatter(mean_lat, mean_long, marker='*', s=150, c='magenta', edgecolors='k', label='grid search start point.')\n",
    "    plt.axvspan(37.95, 37.98, color='coral', alpha=0.25, zorder=0)\n",
    "    plt.axhspan(23.72, 23.76, color='coral', alpha=0.25, zorder=0)\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15),\n",
    "          fancybox=True, shadow=True, ncol=2)\n",
    "    plt.xlabel('Latitude')\n",
    "    plt.ylabel('Longitude')\n",
    "    plt.title('The grid of candidate coordinates.')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def select_nodes_around_a_synergetic_node(elements, from_lat=37.95, to_lat=37.98):\n",
    "    # Select a set of antagonistic elements around a synergetic node\n",
    "    nodes_of_interest_lat, nodes_of_interest_long=([] for i in range(0,2))\n",
    "    for idx,node in enumerate(elements['ant']['lat']):\n",
    "        if node>from_lat and node<to_lat:\n",
    "            nodes_of_interest_lat.append(node)\n",
    "            nodes_of_interest_long.append(elements['ant']['long'][idx])\n",
    "\n",
    "    mean_lat=np.mean(nodes_of_interest_lat)\n",
    "    mean_long=np.mean(nodes_of_interest_long)\n",
    "    \n",
    "    return mean_lat, mean_long\n",
    "    \n",
    "def define_radius_of_interest(elements):\n",
    "    '''\n",
    "    The objective is to minimize the distance from a synergetic node while \n",
    "    maximizing the distance from all the other antagonistic elements. \n",
    "    '''\n",
    "\n",
    "\n",
    "    # Define the range of all possible latitude values \n",
    "    lat_range  =np.linspace(37.91,38.02,51)\n",
    "    long_range =np.linspace(23.7,23.8,51)\n",
    "    \n",
    "    # Construct the grid (Grid of possible establishments)\n",
    "    candidate_lat, canditate_long = np.meshgrid(lat_range, long_range, sparse=False)\n",
    "    # Get the closest nodes around a synergetic node based on a latitude range\n",
    "    mean_lat, mean_long=select_nodes_around_a_synergetic_node(elements)\n",
    "    # plot the coordinates for visual search\n",
    "    plot_superimposed_grid(elements, candidate_lat, canditate_long, mean_lat, mean_long)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# split the companies into synergetic and antagonistic\n",
    "elements= parse_synergetic_and_antagonistic_elements(centroids_coords)  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
